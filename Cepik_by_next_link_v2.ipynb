{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6532bf41-4794-4b5e-a251-09a39e78150b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download from Cepik api for motoliczby.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab43291-72d0-42da-b4e1-02abffcf9a74",
   "metadata": {},
   "source": [
    "Dokumnetacja APi Cepik: https://api.cepik.gov.pl/doc\n",
    "Filtrowanie: filter[<nazwa atrybutu>]=<szukana wartość>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa946fd-b4db-4125-ab1b-b0a48e1350f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4826858c-3037-4430-9ff4-8e05e27a47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of Wojewodztwa\n",
    "wojew_names = {\n",
    "\"02\" : \"DOLNOŚLĄSKIE\",\n",
    "\"04\" : \"KUJAWSKO-POMORSKIE\",\n",
    "\"06\" : \"LUBELSKIE\",\n",
    "\"08\" : \"LUBUSKIE\",\n",
    "\"10\" : \"ŁÓDZKIE\",\n",
    "\"12\" : \"MAŁOPOLSKIE\",\n",
    "\"14\" : \"MAZOWIECKIE\",\n",
    "\"16\" : \"OPOLSKIE\",\n",
    "\"18\" : \"PODKARPACKIE\",\n",
    "\"20\" : \"PODLASKIE\",\n",
    "\"22\" : \"POMORSKIE\",\n",
    "\"24\" : \"ŚLĄSKIE\",\n",
    "\"26\" : \"ŚWIĘTOKRZYSKIE\",\n",
    "\"28\" : \"WARMIŃSKO-MAZURSKIE\",\n",
    "\"30\" : \"WIELKOPOLSKIE\",\n",
    "\"32\" : \"ZACHODNIOPOMORSKIE\",\n",
    "\"XX\" : \"NIEOKREŚLONE\"\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efbe66c-c5c5-47f5-a22f-913a0cd9b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CEPIK API parameters\n",
    "#wojew = [\"02\",\"04\",\"06\",\"08\",\"10\",\"12\",\"14\",\"16\",\"18\",\"20\",\"22\",\"24\",\"26\",\"28\",\"30\",\"32\",\"XX\"] # Wojewodztwa backup \n",
    "\n",
    "wojew = [\"02\",\"04\",\"06\",\"08\",\"10\",\"12\",\"14\",\"16\",\"18\",\"20\",\"22\",\"24\",\"26\",\"28\",\"30\",\"32\",\"XX\"] \n",
    "years = [\"2024\"]\n",
    "MDStart = \"0101\" # mont-data start  0101 as 01 JAN\n",
    "MDEnd = \"1231\" # mont-data end 1231 as 31 DEC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de3f1d7-111d-430d-b608-717b3b17dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings folders\n",
    "\n",
    "main_data_path = \"D:/Dokumenty/Tomek/Data_science/Cepik/nowe_samochody\" # please define here a upload path.\n",
    "current_dic = \"/motoliczby\" # Define a folder\n",
    "now_file = \"Api_download_\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") # Donwload file name pattern \n",
    "upload_path = main_data_path + current_dic + \"_\" + now_file\n",
    "\n",
    "base_name_xls = \"Cepik_\" # base name of the xls/csv file\n",
    "path_to_merge = upload_path + current_dic + \"all/\" # name patern of sumerised date file\n",
    "all_merged_data_file_name =\"Cepik_all_data_in_one_file.csv\" # Name of the file with all data\n",
    "save_total_of_year = False # collect and save all data within a year. It may contain lot of records, slow down the process or make an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d15636d-8a40-40fb-bded-37162f90aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url and filters\n",
    "\n",
    "based_url = \"https://api.cepik.gov.pl/pojazdy?&limit=500\" \n",
    "filtr = '&pokaz-wszystkie-pola=true&sort=-id&tylko-zarejestrowane=false'\n",
    "\n",
    "#Backuped filters\n",
    "#filtr = '&pokaz-wszystkie-pola=true&sort=-id&tylko-zarejestrowane=true'\n",
    "#filtr = '&pokaz-wszystkie-pola=true&sort=-id&tylko-zarejestrowane=false&filter[rodzaj-pojazdu]=Motocykl'\n",
    "#filtr = '&pokaz-wszystkie-pola=true&sort=-id&tylko-zarejestrowane=false&filter[rodzaj-pojazdu]=Motocykl&filter[marka]=bmw&filter[model]=K1'\n",
    "#filtr = '&pokaz-wszystkie-pola=true&sort=-id&tylko-zarejestrowane=false&filter[rodzaj-pojazdu]=Motocykl'\n",
    "\n",
    "#filtr = '&filter[pochodzenie-pojazdu]=NOWY+&tylko-zarejestrowane=false&pokaz-wszystkie-pola=true'\n",
    "#filtr = '&filter[pochodzenie-pojazdu]=NOWY&tylko-zarejestrowane=false&pokaz-wszystkie-pola=true'\n",
    "#filtr = '&tylko-zarejestrowane=false&pokaz-wszystkie-pola=true'\n",
    "#filtr = '&filter[pochodzenie-pojazdu]=NOWY+ZAKUPIONY+W+KRAJU&filter[rodzaj-pojazdu]=SAMOCHÓD+OSOBOWY&pokaz-wszystkie-pola=true&sort=marka&sort=model&sort=typ&sort=wariant'\n",
    "#filtr = '&filter[pochodzenie-pojazdu]=NOWY+ZAKUPIONY+W+KRAJU&filter[pochodzenie-pojazdu]=NOWY+IMPORT+INDYW&filter[pochodzenie-pojazdu]=NOWY+ZAKUPIONY+W+KRAJU&pokaz-wszystkie-pola=true&sort=id'\n",
    "#filtr = '&pokaz-wszystkie-pola=true&sort=-id&tylko-zarejestrowane=false'\n",
    "\n",
    "#elektryczne\n",
    "#filtr = '&pokaz-wszystkie-pola=true&sort=-id&tylko-zarejestrowane=false&filter[rodzaj-paliwa]=ENERGIA+ELEKTRYCZNA'\n",
    "#filtr =\"&filter[rodzaj-pojazdu]=SAMOCHÓD+OSOBOWY\"\n",
    "#filtr = '&filter[marka]=TOYOTA&filter[pochodzenie-pojazdu]=NOWY+ZAKUPIONY+W+KRAJU&filter[pochodzenie-pojazdu]=NOWY+IMPORT+INDYW&tylko-zarejestrowane=false&pokaz-wszystkie-pola=true'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "307cadb0-1b50-4274-aa52-e6f0f01ec9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no df to del\n"
     ]
    }
   ],
   "source": [
    "# data frames declaration\n",
    "all_cars_in_years = pd.DataFrame()\n",
    "cars_in_years_and_woj = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    del cars, df\n",
    "except Exception:\n",
    "    print(\"no df to del\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1e6ca-ec73-47fa-a574-5eb9388233d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ba069-3922-45d7-aa00-0a2a7b9e4e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main Cell\n",
    "\n",
    "# years loop\n",
    "for y in years:\n",
    "#for w in wojew:\n",
    "\n",
    "    print (\"-\"*40)\n",
    "    print (\"Scope of data capture (years): start:\", years[0] ,\"end:\", years[-1] )\n",
    "    print(\"Year:\", y)\n",
    "    \n",
    "    # Wojewodztwo loop\n",
    "    for w in wojew:\n",
    "                    \n",
    "        # variables\n",
    "        wojewodz = \"&wojewodztwo=\" + w\n",
    "        period = \"&data-od=\" + y + MDStart+ \"&data-do=\" + y + MDEnd\n",
    "        path_to_save = upload_path + \"/\" + y + \"/\" + w + \"/\" \n",
    "        path_to_save_all = upload_path + \"/\" + y + \"/\" \n",
    "        \n",
    "        rec_no = 0\n",
    "        step = 1\n",
    "        finished = False \n",
    "        number_of_waits = 0\n",
    "        page = \"&page=1\" \n",
    "        \n",
    "    \n",
    "        #prints \n",
    "        print (\"-\"*20)\n",
    "        print(\"Woj:\", w,\" \",wojew_names[w], \"/\",y)\n",
    "        \n",
    "               \n",
    "        # define first page\n",
    "        url = based_url + period +  filtr + wojewodz + page\n",
    "        \n",
    "        # scrapping loop within the Woj\n",
    "        while not finished:\n",
    "            \n",
    "            print (\"We dig in:\", url)\n",
    "\n",
    "            #------------------------------------    \n",
    "            data_temp = requests.get(url) \n",
    "            #------------------------------------\n",
    "            \n",
    "            # check the respond code if no errors\n",
    "            if data_temp.status_code != 200: # 200 is correct answer (no errors)\n",
    "                number_of_waits = number_of_waits + 1\n",
    "                print (\"digging status NOK: \", data_temp.status_code, \"wait for 5 sek.\", number_of_waits , \"times\")\n",
    "                tc_retry = True\n",
    "                time.sleep(5) # Wait for 5 sek (I know this doesne't help much)\n",
    "                \n",
    "            else:\n",
    "                tc_retry = False\n",
    "                \n",
    "                #print (\"digging OK -\", \"data statatus code:\",data_temp.status_code)\n",
    "                data = data_temp.json()\n",
    "                df = pd.json_normalize(data, record_path =['data'])\n",
    "\n",
    "                # how many times we need to iterate to download all Wojewodztwo\n",
    "                tc_links = pd.json_normalize(data)\n",
    "                                  \n",
    "                # numbers of records\n",
    "                rec_no = tc_links[\"meta.count\"][0]\n",
    "                records_tc =  rec_no\n",
    "                rec_no = math.ceil(rec_no/500)\n",
    "                                \n",
    "                # Stop the loop if there is no records to downlaod\n",
    "                if records_tc == 0:\n",
    "                    print (\"----NO RECORDS DO DOWNLOAD\",\"page/step: \",step,\"/\", rec_no,\" Records in woj: \",w, \"records to download:\",records_tc ,\" pages remaining: \", rec_no - step)\n",
    "                    break;\n",
    "                                \n",
    "                # Make a directory if thera are records to save\n",
    "                if records_tc != 0 and step == 1:\n",
    "                    try: \n",
    "                        os.makedirs (path_to_save)\n",
    "                        print (path_to_save, \" successfully created\")\n",
    "                    except Exception:\n",
    "                        print(\"dic: \", path_to_save, \"aready exist\" )\n",
    "\n",
    "                \n",
    "                # Define next url to capture\n",
    "                try: \n",
    "                    next_url = tc_links[\"links.next\"][0]\n",
    "                except Exception:\n",
    "                    next_url = \"No next url - that was the last one\"\n",
    "                    \n",
    "                # define next url to download    \n",
    "                last_url = tc_links[\"links.last\"][0]\n",
    "                              \n",
    "                # ----------- append data into dataframes of woj ----------------\n",
    "                cars_in_years_and_woj = pd.concat([cars_in_years_and_woj, df])\n",
    "                \n",
    "                # Printouts\n",
    "                print(f\">>page/step: {step}/{rec_no} Records in woj: {w} records to download: {records_tc} total rec: {len(cars_in_years_and_woj)} Unique: {cars_in_years_and_woj['id'].nunique()} pages remaining: {rec_no - step}\")\n",
    "                \n",
    "                # save each set of data from particular step steps into the csv\n",
    "                now_file = \"generated_\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "                #name_xls = path_to_save + base_name_xls + y + \"_woj_\" +w + \"_step_\" + str(step) + \"-\" + now_file + \".csv\"\n",
    "                #name_xls_total = path_to_save_all + base_name_xls + y + \"_woj_\" +w + \"_total_\" + \"-\" + now_file + \".csv\"\n",
    "                name_xls = f\"{path_to_save}{base_name_xls}{y}_woj_{w}_step_{step}-{now_file}.csv\"\n",
    "                name_xls_total = f\"{path_to_save_all}{base_name_xls}{y}_woj_{w}_total_-{now_file}.csv\"\n",
    "\n",
    "                \n",
    "                df.to_csv(path_or_buf = name_xls,\n",
    "                            index = False,\n",
    "                            sep = \";\",\n",
    "                            encoding = 'utf-8')\n",
    "             \n",
    "\n",
    "                # end of wojewodztwo\n",
    "                if last_url == url or next_url == \"No next url - it was last one\":\n",
    "                    \n",
    "                    \n",
    "                    finished = True\n",
    "\n",
    "                    # Save df                 \n",
    "                    cars_in_years_and_woj.to_csv(\n",
    "                                        path_or_buf= name_xls_total,\n",
    "                                        index = False,\n",
    "                                        sep = \";\",\n",
    "                                        encoding = 'utf-8')\n",
    "                    \n",
    "                    # printouts end of the wojewodztwo\n",
    "                    print(\"*\" * 60)\n",
    "                    print(f\"End of Województwo. Województwo {w}/{y} is downloaded\")\n",
    "                    print(f\"File saved: total rec: {len(cars_in_years_and_woj)} Unique: {cars_in_years_and_woj['id'].nunique()}\")\n",
    "                    print(f\">>>>>>>> Duplicates found: {len(cars_in_years_and_woj) - cars_in_years_and_woj['id'].nunique()}\")\n",
    "                    print(\"*\" * 60)\n",
    "\n",
    "\n",
    "                    if save_total_of_year == True:\n",
    "                        # ----------- append data into dataframes of year ----------------\n",
    "                        #all_cars_in_years = all_cars_in_years.append(cars_in_years_and_woj)\n",
    "                        all_cars_in_years = pd.concat([all_cars_in_years , cars_in_years_and_woj])\n",
    "\n",
    "                    # Remove all data from df\n",
    "                    cars_in_years_and_woj = cars_in_years_and_woj.iloc[0:0]\n",
    "  \n",
    "                else:    \n",
    "                    step = step + 1\n",
    "                    url = next_url\n",
    "                print (\"-\"*5)\n",
    "  \n",
    "    if save_total_of_year == True:\n",
    "    \n",
    "        # Save all data within a year   \n",
    "        if len(cars_in_years_and_woj) > 0:\n",
    "\n",
    "            # Verification of duplicates\n",
    "            cars_in_years_and_woj_records = len(cars_in_years_and_woj)\n",
    "            cars_in_years_and_woj_unique = cars_in_years_and_woj['id'].nunique()\n",
    "\n",
    "            cars_in_years_and_woj.to_csv(\n",
    "                path_or_buf= path_to_save_all + base_name_xls + y + \"_all_woj_total_\" + \"-\" + now_file + \".csv\",\n",
    "                index = False,\n",
    "                sep = \";\",\n",
    "                encoding = 'utf-8')\n",
    "\n",
    "\n",
    "            # Printouts end of the year\n",
    "            \n",
    "            print(\"-------------------------------------\")\n",
    "            print(f\"------ Year {y} Successfully downloaded to {path_to_save_all}\")\n",
    "            print(f\"Total records: {cars_in_years_and_woj_records}\")\n",
    "            print(f\"Total unique: {cars_in_years_and_woj_unique}\")\n",
    "\n",
    "            if cars_in_years_and_woj_records == cars_in_years_and_woj_unique:\n",
    "                print(\"OK, no duplicates found\")\n",
    "            else:\n",
    "                print(\"!!!! Possible duplicates, please check it!\")\n",
    "\n",
    "            print(\"-------------------------------------\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(\"No records found in: \",y,\" year\")\n",
    "\n",
    "        # end of Year loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10dec2-4618-46f3-b967-b3169340ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"koniec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4581f-6e34-451c-907d-d0d2c2455521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2193a6-e110-4d8e-bebc-0dc070d1b267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0aabfd-ccec-4b04-a5e2-e25d76c298ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed1a06-227c-4ae8-b607-bf9e69823a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e7342ba0ab402af88d570c19a0b1c8a8c78f2edbd6d0204fb59a98d1c2d2ef0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
